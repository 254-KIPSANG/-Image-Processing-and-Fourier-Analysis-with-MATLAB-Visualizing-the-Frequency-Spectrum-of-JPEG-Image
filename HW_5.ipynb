{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/254-KIPSANG/-Image-Processing-and-Fourier-Analysis-with-MATLAB-Visualizing-the-Frequency-Spectrum-of-JPEG-Image/blob/main/HW_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkEOlue6keT0"
      },
      "source": [
        "# HW 5: Clustering and Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzO-TgiRkeT2"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">Each assignment needs to be completed independently. Never ever copy others' work (even with minor modification, e.g. changing variable names). Anti-Plagiarism software will be used to check all submissions. </div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCCoJu3KkeT3"
      },
      "source": [
        "In this assignment, you'll need to use the following dataset:\n",
        "- text_train.json: This file contains a list of documents. It's used for training models\n",
        "- text_test.json: This file contains a list of documents and their ground-truth labels. It's used for testing performance. This file is in the format shown below. Note, a document may have multiple labels.\n",
        "\n",
        "\n",
        "**Note: due to randomness, every time you run your clustering models, you may get different results. To ease the grading process, once you get satisfactory results, please save your notebook as a pdf file (Jupyter notebook menu File -> Print -> Save as pdf), and submit this pdf along with your .py code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao9KR_FAkeT3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Necessary imports for Q1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Add your import statement\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMIjaDx3keT4",
        "outputId": "017f0fca-6a40-4dbe-f128-b55e50c7025b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-32613746266c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hw5_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hw5_test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hw5_train.csv'"
          ]
        }
      ],
      "source": [
        "train_data = pd.read_csv(\"hw5_train.csv\")\n",
        "train_data.head()\n",
        "\n",
        "test_data = pd.read_csv(\"hw5_test.csv\")\n",
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y97afImkkeT5"
      },
      "source": [
        "## Q1: K-Mean Clustering (5 points)\n",
        "\n",
        "Define a function `cluster_kmean(train_data, test_data, num_clusters, min_df = 1, stopwords = None, metric = 'cosine')` as follows: \n",
        "- Take two dataframes as inputs: `train_data` is the dataframe loaded from `hw5_train.csv`, and `test_data` is the dataframe loaded from `hw5_test.csv`\n",
        "- Use **KMeans** to cluster documents in `train_data` into 3 clusters by the distance metric specified. Tune the following parameters carefully:\n",
        "    - `min_df` and `stopword` options in generating TFIDF matrix. You may need to remove corpus-specific stopwords in addition to the standard stopwords.\n",
        "    - distance metric: `cosine` or `Euclidean` distance\n",
        "    - sufficient iterations with different initial centroids to make sure clustering converges\n",
        "- Test the clustering model performance using `test_data`: \n",
        "    - Predict the cluster ID for each document in `test_data`.\n",
        "    - Apply `majority vote` rule to dynamically map each cluster to a ground-truth label in `test_data`. \n",
        "        - Note a small percentage of documents have multiple labels. For these cases, you can randomly pick a label during the match\n",
        "        - Be sure `not to hardcode the mapping`, because a  cluster may corrspond to a different topic in each run. (hint: if you use pandas, look for `idxmax` function)\n",
        "    - Calculate `precision/recall/f-score` for each label. Your best F1 score on the test dataset should be around `80%`.\n",
        "- Assign a meaninful name to each cluster based on the `top keywords` in each cluster. You can print out the keywords and write the cluster names as markdown comments.\n",
        "- This function has no return. Print out confusion matrix, precision/recall/f-score. \n",
        "\n",
        "\n",
        "**Analysis**:\n",
        "- Comparing the clustering with cosine distance and that with Euclidean distance, do you notice any difference? Which metric works better here?\n",
        "- How would the stopwords and min_df options affect your clustering results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pah-AqKPkeT5"
      },
      "outputs": [],
      "source": [
        "def cluster_kmean(train_data, test_data, num_clusters, min_df=1, stopwords=None, metric='cosine'):\n",
        "    # Preprocess train_data\n",
        "    train_docs = preprocess_documents(train_data['text'], stopwords)\n",
        "\n",
        "    # Create TF-IDF matrix\n",
        "    tfidf_vectorizer = TfidfVectorizer(min_df=min_df, stop_words=stopwords)\n",
        "    train_tfidf_matrix = tfidf_vectorizer.fit_transform(train_docs)\n",
        "\n",
        "    # Cluster documents using KMeans\n",
        "    kmeans = KMeans(n_clusters=num_clusters, init='k-means++', n_init=10, max_iter=300, random_state=0)\n",
        "    kmeans.fit(train_tfidf_matrix)\n",
        "\n",
        "    # Preprocess test_data\n",
        "    test_docs = preprocess_documents(test_data['text'], stopwords)\n",
        "\n",
        "    # Transform test_data to TF-IDF matrix\n",
        "    test_tfidf_matrix = tfidf_vectorizer.transform(test_docs)\n",
        "\n",
        "    # Predict cluster IDs for test_data\n",
        "    test_cluster_ids = kmeans.predict(test_tfidf_matrix)\n",
        "\n",
        "    # Majority vote to map cluster to ground-truth label\n",
        "    cluster_labels = {}\n",
        "    for cluster_id in range(num_clusters):\n",
        "        cluster_docs_indices = np.where(test_cluster_ids == cluster_id)[0]\n",
        "        cluster_docs = test_data.iloc[cluster_docs_indices]\n",
        "        cluster_label_counts = cluster_docs['labels'].str.split(';').apply(pd.Series).stack().value_counts()\n",
        "        cluster_label = cluster_label_counts.idxmax()\n",
        "        cluster_labels[cluster_id] = cluster_label\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    ground_truth = mlb.fit_transform(test_data['labels'].str.split(';'))\n",
        "    predicted_labels = [cluster_labels[cluster_id] for cluster_id in test_cluster_ids]\n",
        "    predicted_labels = mlb.fit_transform(predicted_labels)\n",
        "\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(ground_truth, predicted_labels, average='weighted')\n",
        "\n",
        "    # Print performance metrics\n",
        "    print(\"Performance Metrics:\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F-score: \", fscore)\n",
        "\n",
        "    # Assign cluster names based on top keywords\n",
        "    print(\"\\nCluster Names:\")\n",
        "    for i in range(num_clusters):\n",
        "        cluster_keywords = get_top_keywords(tfidf_vectorizer, kmeans.cluster_centers_[i], 5)\n",
        "        cluster_name = \"Cluster \" + str(i + 1) + \": \" + \", \".join(cluster_keywords)\n",
        "        print(cluster_name)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    confusion_mat = confusion_matrix(ground_truth.argmax(axis=1), predicted_labels.argmax(axis=1))\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(confusion_mat, annot=True, cmap=\"YlGnBu\", cbar=False, xticklabels=mlb.classes_, yticklabels=mlb.classes_)\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('Ground Truth Labels')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpRbe1WHkeT6"
      },
      "outputs": [],
      "source": [
        "# Clustering by cosine distance\n",
        "\n",
        "\n",
        "def cluster_kmean(train_data, test_data, num_clusters, min_df=1, stopwords=None, metric='cosine'):\n",
        "    # Preprocess train_data\n",
        "    train_docs = preprocess_documents(train_data['text'], stopwords)\n",
        "\n",
        "    # Create TF-IDF matrix\n",
        "    tfidf_vectorizer = TfidfVectorizer(min_df=min_df, stop_words=stopwords)\n",
        "    train_tfidf_matrix = tfidf_vectorizer.fit_transform(train_docs)\n",
        "\n",
        "    # Cluster documents using KMeans with cosine distance\n",
        "    if metric == 'cosine':\n",
        "        kmeans = KMeans(n_clusters=num_clusters, init='k-means++', n_init=10, max_iter=300, random_state=0, \n",
        "                        verbose=0, algorithm='full', precompute_distances='auto', metric='cosine')\n",
        "    else:\n",
        "        # Cluster documents using KMeans with Euclidean distance\n",
        "        kmeans = KMeans(n_clusters=num_clusters, init='k-means++', n_init=10, max_iter=300, random_state=0, \n",
        "                        verbose=0, algorithm='auto')\n",
        "    \n",
        "    kmeans.fit(train_tfidf_matrix)\n",
        "\n",
        "    # Preprocess test_data\n",
        "    test_docs = preprocess_documents(test_data['text'], stopwords)\n",
        "\n",
        "    # Transform test_data to TF-IDF matrix\n",
        "    test_tfidf_matrix = tfidf_vectorizer.transform(test_docs)\n",
        "\n",
        "    # Predict cluster IDs for test_data\n",
        "    test_cluster_ids = kmeans.predict(test_tfidf_matrix)\n",
        "\n",
        "    # Majority vote to map cluster to ground-truth label\n",
        "    cluster_labels = {}\n",
        "    for cluster_id in range(num_clusters):\n",
        "        cluster_docs_indices = np.where(test_cluster_ids == cluster_id)[0]\n",
        "        cluster_docs = test_data.iloc[cluster_docs_indices]\n",
        "        cluster_label_counts = cluster_docs['labels'].str.split(';').apply(pd.Series).stack().value_counts()\n",
        "        cluster_label = cluster_label_counts.idxmax()\n",
        "        cluster_labels[cluster_id] = cluster_label\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    ground_truth = mlb.fit_transform(test_data['labels'].str.split(';'))\n",
        "    predicted_labels = [cluster_labels[cluster_id] for cluster_id in test_cluster_ids]\n",
        "    predicted_labels = mlb.fit_transform(predicted_labels)\n",
        "\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(ground_truth, predicted_labels, average='weighted')\n",
        "\n",
        "    # Print performance metrics\n",
        "    print(\"Performance Metrics:\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F-score: \", fscore)\n",
        "\n",
        "    # Assign cluster names based on top keywords\n",
        "    print(\"\\nCluster Names:\")\n",
        "    for i in range(num_clusters):\n",
        "        cluster_keywords = get_top_keywords(tfidf_vectorizer, kmeans.cluster_centers_[i], 5)\n",
        "        cluster_name = \"Cluster \" + str(i + 1) + \": \" + \", \".join(cluster_keywords)\n",
        "        print(cluster_name)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    confusion_mat = confusion_matrix(ground_truth.argmax(axis=1), predicted_labels.argmax(axis=1))\n",
        "    print(pd.DataFrame(confusion_mat, index=['Cluster 0', 'Cluster 1', 'Cluster 2'], columns=['T1', 'T2', 'T3']))\n",
        "    print(classification_report(ground_truth.argmax(axis=1), predicted_labels.argmax(axis=1), target_names=['T1', 'T2', 'T3']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BCby4jBkeT6"
      },
      "outputs": [],
      "source": [
        "# Clustering by Euclidean distance\n",
        "\n",
        "def cluster_kmean(train_data, test_data, num_clusters, min_df=1, stopwords=None, metric='euclidean'):\n",
        "    # Preprocess train_data\n",
        "    train_docs = preprocess_documents(train_data['text'], stopwords)\n",
        "\n",
        "    # Create TF-IDF matrix\n",
        "    tfidf_vectorizer = TfidfVectorizer(min_df=min_df, stop_words=stopwords)\n",
        "    train_tfidf_matrix = tfidf_vectorizer.fit_transform(train_docs)\n",
        "\n",
        "    # Cluster documents using KMeans with Euclidean distance\n",
        "    if metric == 'cosine':\n",
        "        # Cluster documents using KMeans with cosine distance\n",
        "        kmeans = KMeans(n_clusters=num_clusters, init='k-means++', n_init=10, max_iter=300, random_state=0, \n",
        "                        verbose=0, algorithm='full', precompute_distances='auto')\n",
        "    else:\n",
        "        kmeans = KMeans(n_clusters=num_clusters, init='k-means++', n_init=10, max_iter=300, random_state=0, \n",
        "                        verbose=0, algorithm='auto')\n",
        "    \n",
        "    kmeans.fit(train_tfidf_matrix)\n",
        "\n",
        "    # Preprocess test_data\n",
        "    test_docs = preprocess_documents(test_data['text'], stopwords)\n",
        "\n",
        "    # Transform test_data to TF-IDF matrix\n",
        "    test_tfidf_matrix = tfidf_vectorizer.transform(test_docs)\n",
        "\n",
        "    # Predict cluster IDs for test_data\n",
        "    test_cluster_ids = kmeans.predict(test_tfidf_matrix)\n",
        "\n",
        "    # Majority vote to map cluster to ground-truth label\n",
        "    cluster_labels = {}\n",
        "    for cluster_id in range(num_clusters):\n",
        "        cluster_docs_indices = np.where(test_cluster_ids == cluster_id)[0]\n",
        "        cluster_docs = test_data.iloc[cluster_docs_indices]\n",
        "        cluster_label_counts = cluster_docs['labels'].str.split(';').apply(pd.Series).stack().value_counts()\n",
        "        cluster_label = cluster_label_counts.idxmax()\n",
        "        cluster_labels[cluster_id] = cluster_label\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    ground_truth = mlb.fit_transform(test_data['labels'].str.split(';'))\n",
        "    predicted_labels = [cluster_labels[cluster_id] for cluster_id in test_cluster_ids]\n",
        "    predicted_labels = mlb.fit_transform(predicted_labels)\n",
        "\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(ground_truth, predicted_labels, average='weighted')\n",
        "\n",
        "    # Print performance metrics\n",
        "    print(\"Performance Metrics:\")\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"Recall: \", recall)\n",
        "    print(\"F-score: \", fscore)\n",
        "\n",
        "    # Assign cluster names based on top keywords\n",
        "    print(\"\\nCluster Names:\")\n",
        "    for i in range(num_clusters):\n",
        "        cluster_keywords = get_top_keywords(tfidf_vectorizer, kmeans.cluster_centers_[i], 5)\n",
        "        cluster_name = \"Cluster \" + str(i + 1) + \": \" + \", \".join(cluster_keywords)\n",
        "        print(cluster_name)\n",
        "\n",
        "    # Print confusion matrix\n",
        "    confusion_mat = pd.crosstab(test_data['labels'].str.split(';').apply(pd.Series).stack().reset_index(level=1, drop=True), \n",
        "                                [cluster_labels[cluster_id] for cluster_id in test_cluster_ids], \n",
        "                                rownames=['Actual'], colnames=['Cluster'])\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_mat)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3irf_NnkeT7"
      },
      "source": [
        "## Q2: GMM Clustering (5 points)\n",
        "\n",
        "Define a function `cluster_gmm(train_data, test_data, num_clusters, min_df = 10, stopwords = stopwords)`  to redo Q1 using the Gaussian mixture model. \n",
        "\n",
        "**Requirements**:\n",
        "\n",
        "- To save time, you can specify the covariance type as `diag`.\n",
        "- Be sure to run the clustering with different initiations to get stabel clustering results\n",
        "- Your F1 score on the test set should be around `70%` or higher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34TAkAL_keT7"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import random\n",
        "\n",
        "def cluster_gmm(train_data, test_data, num_clusters, min_df=10, stopwords=None):\n",
        "    \"\"\"\n",
        "    Cluster documents in train_data using Gaussian Mixture Model (GMM) and evaluate the clustering performance using test_data.\n",
        "\n",
        "    Args:\n",
        "        train_data (pd.DataFrame): DataFrame containing the training data.\n",
        "        test_data (pd.DataFrame): DataFrame containing the test data.\n",
        "        num_clusters (int): Number of clusters to form.\n",
        "        min_df (int, optional): Minimum document frequency threshold for TfidfVectorizer. Defaults to 10.\n",
        "        stopwords (list, optional): List of stopwords to be removed during text preprocessing. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Preprocess text data\n",
        "    vectorizer = TfidfVectorizer(min_df=min_df, stop_words=stopwords)\n",
        "    train_docs = vectorizer.fit_transform(train_data['text'])\n",
        "    test_docs = vectorizer.transform(test_data['text'])\n",
        "    \n",
        "    # Cluster using Gaussian Mixture Model (GMM)\n",
        "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='diag', random_state=0)\n",
        "    gmm.fit(train_docs.toarray())\n",
        "    train_preds = gmm.predict(train_docs.toarray())\n",
        "    test_preds = gmm.predict(test_docs.toarray())\n",
        "    \n",
        "    # Assign cluster labels to ground-truth labels using majority vote\n",
        "    train_data['cluster'] = train_preds\n",
        "    cluster_labels = train_data.groupby('cluster')['label'].apply(lambda x: random.choice(x.value_counts().index))\n",
        "    test_data['predicted_label'] = test_preds\n",
        "    test_data['predicted_label'] = test_data['predicted_label'].map(cluster_labels)\n",
        "    \n",
        "    # Calculate precision, recall, and F1-score for each label\n",
        "    print(confusion_matrix(test_data['label'], test_data['predicted_label']))\n",
        "    print(classification_report(test_data['label'], test_data['predicted_label']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ3Fxe0AkeT8",
        "outputId": "9afdd700-0b45-4212-d5c2-913921b558c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 22   0   0   0   0   0   0   0   0   0   0 129   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [189   0   0   0   0   0   0   0   0   0   0  13   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [187   0   0   0   0   0   0   0   0   0   0   8   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [180   0   0   0   0   0   0   0   0   0   0   3   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [198   0   0   0   0   0   0   0   0   0   0   7   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [205   0   0   0   0   0   0   0   0   0   0  10   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [185   0   0   0   0   0   0   0   0   0   0   8   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [149   0   0   0   0   0   0   0   0   0   0  47   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [117   0   0   0   0   0   0   0   0   0   0  51   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [156   0   0   0   0   0   0   0   0   0   0  55   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [133   0   0   0   0   0   0   0   0   0   0  65   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [ 69   0   0   0   0   0   0   0   0   0   0 132   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [177   0   0   0   0   0   0   0   0   0   0  25   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [ 70   0   0   0   0   0   0   0   0   0   0 124   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [126   0   0   0   0   0   0   0   0   0   0  63   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [ 19   0   0   0   0   0   0   0   0   0   0 183   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [ 39   0   0   0   0   0   0   0   0   0   0 149   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [ 22   0   0   0   0   0   0   0   0   0   0 160   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [ 32   0   0   0   0   0   0   0   0   0   0 127   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [ 27   0   0   0   0   0   0   0   0   0   0 109   0   0   0   0   0   0\n",
            "    0   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.01      0.15      0.02       151\n",
            "           1       0.00      0.00      0.00       202\n",
            "           2       0.00      0.00      0.00       195\n",
            "           3       0.00      0.00      0.00       183\n",
            "           4       0.00      0.00      0.00       205\n",
            "           5       0.00      0.00      0.00       215\n",
            "           6       0.00      0.00      0.00       193\n",
            "           7       0.00      0.00      0.00       196\n",
            "           8       0.00      0.00      0.00       168\n",
            "           9       0.00      0.00      0.00       211\n",
            "          10       0.00      0.00      0.00       198\n",
            "          11       0.09      0.66      0.16       201\n",
            "          12       0.00      0.00      0.00       202\n",
            "          13       0.00      0.00      0.00       194\n",
            "          14       0.00      0.00      0.00       189\n",
            "          15       0.00      0.00      0.00       202\n",
            "          16       0.00      0.00      0.00       188\n",
            "          17       0.00      0.00      0.00       182\n",
            "          18       0.00      0.00      0.00       159\n",
            "          19       0.00      0.00      0.00       136\n",
            "\n",
            "    accuracy                           0.04      3770\n",
            "   macro avg       0.00      0.04      0.01      3770\n",
            "weighted avg       0.01      0.04      0.01      3770\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\KALITOH\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\KALITOH\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "C:\\Users\\KALITOH\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# Test GMM model\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import random\n",
        "\n",
        "# Load the 20 newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "df = pd.DataFrame({'text': newsgroups.data, 'label': newsgroups.target})\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define list of stopwords\n",
        "stopwords = ['the', 'and', 'is', 'in', 'it', 'of', 'to', 'for', 'with', 'on']\n",
        "\n",
        "# Call the cluster_gmm function\n",
        "cluster_gmm(train_data, test_data, num_clusters=3, min_df=10, stopwords=stopwords)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvELQaVFkeT8"
      },
      "source": [
        "## Q3: LDA Clustering (5 points)\n",
        "\n",
        "**Q3.1.** Define a function `cluster_lda(train_data, test_data, num_clusters, min_df = 5, stopwords = stopwords)`  to redo Q1 using the LDA model. Note, for LDA, you need to use `CountVectorizer` instead of `TfidfVectorizer`. \n",
        "\n",
        "**Requirements**:\n",
        "- Your F1 score on the test set should be around `80%` or higher\n",
        "- Print out top-10 words in each topic\n",
        "- Return the topic mixture per document matrix for the test set(denoted as `doc_topics`) and the trained LDA model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sgunUPFkeT8"
      },
      "source": [
        "**Q3.2**. Find similar documents\n",
        "\n",
        "- Define a function `find_similar_doc(doc_id, doc_topics)` to find `top 3 documents` that are the most thematically similar to the document with `doc_id` using the `doc_topics`. (1 point)\n",
        "- Return the IDs of these similar documents.\n",
        "- Print the text of these documents to check if their thematic similarity.\n",
        "\n",
        "\n",
        "**Analysis**:\n",
        "\n",
        "You already learned how to find similar documents by using TFIDF weights. Can you comment on the difference between the approach you just implemented with the one by TFID weights?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcLU6S_bkeT8"
      },
      "outputs": [],
      "source": [
        "def cluster_lda(train_data, test_data, num_clusters, min_df = 5, stopwords = stopwords):\n",
        "    \n",
        "    model, doc_topic = None, None\n",
        "    \n",
        "    # add your code\n",
        "    \n",
        "    \n",
        "    return model, doc_topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOaxEGqqkeT8"
      },
      "outputs": [],
      "source": [
        "# Test LDA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fCv-2chkeT9"
      },
      "outputs": [],
      "source": [
        "def find_similar(doc_id, doc_topics):\n",
        "   \n",
        "    # Add your code\n",
        "    \n",
        "    return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFeEf6tlkeT9",
        "outputId": "217b54a7-89cf-4a7f-f9e6-c1853ef8fdd1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'doc_topics' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7268\\3625225271.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc_topics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'doc_topics' is not defined"
          ]
        }
      ],
      "source": [
        "doc_topics[10:15]\n",
        "\n",
        "doc_id = 11\n",
        "idx = find_similar(doc_id, doc_topics)\n",
        "\n",
        "print(test_data.text.iloc[doc_id])\n",
        "print(\"Similar documents: \\n\")\n",
        "for i in idx:\n",
        "    print(i, test_data.iloc[i].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y04Y6byDkeT9"
      },
      "source": [
        "## Q4 (Bonus): Find the most significant topics in a document\n",
        "\n",
        "A small portion of documents in our dataset have multiple topics. For instace, consider the following document which has topic T2 and T3. The LDA model returns two significant topics with probabilities 0.355 and 0.644. Can you describe a way to find out most significant topics in documents but ignore the insignificant ones? In this example, you should ignore the first topic but keep the last two.\n",
        "\n",
        "- Implement your ideas\n",
        "- Test your ideas with the test set\n",
        "- Recalculate the precision/recall/f1 score for each label.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZCbJo7QkeT9"
      },
      "outputs": [],
      "source": [
        "(test_data.reset_index()).iloc[12:13]\n",
        "doc_topics[12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mL8ze9sskeT9"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":  \n",
        "    \n",
        "    # Due to randomness, you won't get the exact result\n",
        "    # as shown here, but your result should be close\n",
        "    # if you tune the parameters carefully\n",
        "    \n",
        "    # Q1\n",
        "   \n",
        "            \n",
        "    # Q2\n",
        "    \n",
        "    \n",
        "    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}